# -*- coding: utf-8 -*-
"""PINN Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JylpoYp5eGGHfjMHsFPyMJBbsa5sfdXL
"""

import torch
import torch.nn.functional as F


"""
class NavierLossOffset(nn.Module):
    Compute the Navier-Stokes residual loss for 3D incompressible flow.

    Args:
        pred: Tensor of shape (N, 4), where pred[:, 0:3] = (u, v, w), pred[:, 3] = p
        coords: Tensor of shape (N, 4), where coords = (x, y, z, t)
        nu: Kinematic viscosity (scalar)

    Returns:
        continuity_loss: scalar
        momentum_loss: scalar

    def __init__(
        self,
        loss_weight=1.0,
    ):
        super(NavierLossOffset, self).__init__()
        self.loss_weight = loss_weight
    def forward(self, model, coords):
        nu = 0.004
        coords.requires_grad = True
        x, y, z, t = coords[:, 0:1, :], coords[:, 1:2, :], coords[:, 2:3, :], coords[:, 3:4, :]
        pv = torch.cat([x, y, z, t], dim=-2)
        pred = model(pv)
        p, u, v, w = pred[:, 0:1, :], pred[:, 1:2, :], pred[:, 2:3, :], pred[:, 3:4, :]
        coords.requires_grad_(True)

        # Compute gradients
        grads = lambda f: torch.autograd.grad(f, coords, grad_outputs=torch.ones_like(f), create_graph=True)[0]

        # Velocity gradients
        du = grads(u)
        dv = grads(v)
        dw = grads(w)
        dp = grads(p)

        # ∇·u (divergence)
        continuity = du[:, 0] + dv[:, 1] + dw[:, 2]

        # Time derivatives
        du_dt = du[:, 3]
        dv_dt = dv[:, 3]
        dw_dt = dw[:, 3]

        # Spatial gradients for convective terms (u·∇)u
        u_grad = du[:, 0] * u + du[:, 1] * v + du[:, 2] * w
        v_grad = dv[:, 0] * u + dv[:, 1] * v + dv[:, 2] * w
        w_grad = dw[:, 0] * u + dw[:, 1] * v + dw[:, 2] * w

        # Pressure gradients
        dp_dx = dp[:, 0]
        dp_dy = dp[:, 1]
        dp_dz = dp[:, 2]

        # Laplacians (∇²u)
        laplace = lambda grad_f: sum([
            grads(grad_f[:, i])[:, i] for i in range(3)
        ])

        lap_u = laplace(du)
        lap_v = laplace(dv)
        lap_w = laplace(dw)

        # Momentum residuals
        res_u = du_dt + u_grad + dp_dx - nu * lap_u
        res_v = dv_dt + v_grad + dp_dy - nu * lap_v
        res_w = dw_dt + w_grad + dp_dz - nu * lap_w

        # Losses (mean squared)
        continuity_loss = torch.mean(continuity**2)
        momentum_loss = torch.mean(res_u**2 + res_v**2 + res_w**2)

        return continuity_loss, momentum_loss
"""



class NavierLossOffset(nn.Module):
    def __init__(
        self,
        loss_weight=1.0,
    ):
        super(NavierLossOffset, self).__init__()
        self.loss_weight = loss_weight

    def fun_u_0(self, u_x, v_y, w_z):
        return u_x + v_y + w_z

    def fun_r(self, u, v, w, u_t, u_x, u_y, u_z, f_x, u_xx, u_yy, u_zz):
        return u_t + u * u_x + v * u_y + w * u_z + f_x - (u_xx + u_yy + u_zz) / 300

    def check_nan(self, name, tensor):
        if tensor is not None and torch.isnan(tensor).any():
            print(f"NaNs detected in {name}")
        elif tensor is None:
            print(f"{name} is None (gradient may not have been computed correctly)")

    def forward(self, model, feat, is_training = True):
      with torch.set_grad_enabled(is_training):
        feat.requires_grad = is_training
        x, y, z, t = feat[:, 0:1, :], feat[:, 1:2, :], feat[:, 2:3, :], feat[:, 3:4, :]
        pv = torch.cat([x, y, z, t], dim=-2)
        pred = model(pv)
        f, u, v, w = pred[:, 0:1, :], pred[:, 1:2, :], pred[:, 2:3, :], pred[:, 3:4, :]

        # Compute time derivatives
        u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(t), retain_graph=True)[0]
        v_t = torch.autograd.grad(v, t, grad_outputs=torch.ones_like(t), retain_graph=True)[0]
        w_t = torch.autograd.grad(w, t, grad_outputs=torch.ones_like(t), retain_graph=True)[0]

        # Compute spatial derivatives
        u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(x), retain_graph=True, create_graph=True)[0]
        u_y = torch.autograd.grad(u, y, grad_outputs=torch.ones_like(y), retain_graph=True, create_graph=True)[0]
        u_z = torch.autograd.grad(u, z, grad_outputs=torch.ones_like(z), retain_graph=True, create_graph=True)[0]

        v_x = torch.autograd.grad(v, x, grad_outputs=torch.ones_like(x), retain_graph=True, create_graph=True)[0]
        v_y = torch.autograd.grad(v, y, grad_outputs=torch.ones_like(y), retain_graph=True, create_graph=True)[0]
        v_z = torch.autograd.grad(v, z, grad_outputs=torch.ones_like(z), retain_graph=True, create_graph=True)[0]

        w_x = torch.autograd.grad(w, x, grad_outputs=torch.ones_like(x), retain_graph=True, create_graph=True)[0]
        w_y = torch.autograd.grad(w, y, grad_outputs=torch.ones_like(y), retain_graph=True, create_graph=True)[0]
        w_z = torch.autograd.grad(w, z, grad_outputs=torch.ones_like(z), retain_graph=True, create_graph=True)[0]

        f_x = torch.autograd.grad(f, x, grad_outputs=torch.ones_like(x), retain_graph=True)[0]
        f_y = torch.autograd.grad(f, y, grad_outputs=torch.ones_like(y), retain_graph=True)[0]
        f_z = torch.autograd.grad(f, z, grad_outputs=torch.ones_like(z), retain_graph=True)[0]

        # Compute second derivatives
        u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(x), retain_graph=True)[0]
        u_yy = torch.autograd.grad(u_y, y, grad_outputs=torch.ones_like(y), retain_graph=True)[0]
        u_zz = torch.autograd.grad(u_z, z, grad_outputs=torch.ones_like(z), retain_graph=True)[0]

        v_xx = torch.autograd.grad(v_x, x, grad_outputs=torch.ones_like(x), retain_graph=True)[0]
        v_yy = torch.autograd.grad(v_y, y, grad_outputs=torch.ones_like(y), retain_graph=True)[0]
        v_zz = torch.autograd.grad(v_z, z, grad_outputs=torch.ones_like(z), retain_graph=True)[0]

        w_xx = torch.autograd.grad(w_x, x, grad_outputs=torch.ones_like(x), retain_graph=True)[0]
        w_yy = torch.autograd.grad(w_y, y, grad_outputs=torch.ones_like(y), retain_graph=True)[0]
        w_zz = torch.autograd.grad(w_z, z, grad_outputs=torch.ones_like(z), retain_graph=True)[0]

        for name, tensor in zip(["u_xx", "u_yy", "u_zz", "v_xx", "v_yy", "v_zz", "w_xx", "w_yy", "w_zz"],
                                [u_xx, u_yy, u_zz, v_xx, v_yy, v_zz, w_xx, w_yy, w_zz]):
            self.check_nan(name, tensor)

        # Compute PDE residuals
        torch.cuda.empty_cache()
        r1 = self.fun_r(u, v, w, u_t, u_x, u_y, u_z, f_x, u_xx, u_yy, u_zz)
        r2 = self.fun_r(u, v, w, v_t, v_x, v_y, v_z, f_y, v_xx, v_yy, v_zz)
        r3 = self.fun_r(u, v, w, w_t, w_x, w_y, w_z, f_z, w_xx, w_yy, w_zz)
        r4 = self.fun_u_0(u_x, v_y, w_z)

        return self.loss_weight * (torch.mean(r1 ** 2) + torch.mean(r2 ** 2) + torch.mean(r3 ** 2) + torch.mean(r4 ** 2))