# -*- coding: utf-8 -*-
"""Train PINN Segmentation Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UBosPRy0pEg3TPT9AkmKjmNZ_C9SuBgj
"""

!pip install tensorboard



import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
import os
import matplotlib.pyplot as plt


writer = SummaryWriter(log_dir="runs/vascular_experiment_Seg_DGCNN")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

checkpoint_path = "model_checkpoint_Seg_DGCNN.pth"

train_dataset = VascularDataset(data_root=folder_path, category='aorta', point_number=512, num_points_skel=128, split='train')#, random_flip= Ture, random_jitter=True, random_scale=True, random_shift=True, center_shift=True, normalize_coord=True)
val_dataset = VascularDataset(data_root=folder_path, category='aorta', point_number=512, num_points_skel=128, split='val')#, center_shift=True)
test_dataset = VascularDataset(data_root=folder_path, category='aorta', point_number=512, num_points_skel=128, split='test')#, center_shift=True)

train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)
val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)
test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)

use_skeleton_feature = False
if use_skeleton_feature:
  input_dim = 5
else:
  input_dim = 4
model = DGCNN(in_channels=input_dim, out_channels=4).to(device)


criterion1 = nn.MSELoss()
navier_loss = NavierLossOffset(loss_weight=0.01)
use_navier_loss = True

optimizer = optim.Adam(model.parameters(), lr=0.001)

start_epoch = 0
resume = True

if resume and os.path.exists(checkpoint_path):
    print(f"Resuming training from checkpoint: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint["model_state_dict"])
    optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
    start_epoch = checkpoint["epoch"] + 1
    print(f"Resumed training from epoch {start_epoch}")

trainMSE = []
valMSE = []

# TRAINING
num_epochs = 10
for epoch in range(start_epoch, num_epochs):
    model.train()
    running_loss1 = 0.0
    running_loss2 = 0.0
    running_total_loss = 0.0
    batchMSE = []
    batchLoss = []

    with tqdm(total=len(train_dataloader), desc=f'Epoch [{epoch+1}/{num_epochs}]', unit='batch') as pbar:
        for step, batch in enumerate(train_dataloader):
            coordinates = batch['coord'].to(torch.float32).to(device)
            segments = batch['segment'].to(torch.float32).to(device).transpose(1, 2)
            time_steps = batch['time'].to(torch.float32).to(device)
            inputs = torch.cat((coordinates, time_steps), dim=2).transpose(1, 2)


            optimizer.zero_grad()
            outputs = model(inputs)

            loss1 = criterion1(outputs, segments)
            loss2 = navier_loss(model, inputs) if use_navier_loss else torch.tensor(0.0, device=device)
            loss = loss1 + loss2

            loss.backward()
            optimizer.step()


            running_loss1 += loss1.item()
            running_loss2 += loss2.item()
            running_total_loss += loss.item()


            batchMSE.append(loss1.item())



            pbar.set_postfix(loss1=running_loss1 / (pbar.n + 1),
                             loss2=running_loss2 / (pbar.n + 1),
                             total_loss=running_total_loss / (pbar.n + 1))
            pbar.update(1)

            writer.add_scalar("Loss/train_loss1", loss1.item(), step + epoch * len(train_dataloader))
            writer.add_scalar("Loss/train_loss2", loss2.item(), step + epoch * len(train_dataloader))
            writer.add_scalar("Loss/total_loss", loss.item(), step + epoch * len(train_dataloader))

        epochMSE = np.mean(batchMSE)
        trainMSE.append(epochMSE)


    epoch_loss1 = running_loss1 / len(train_dataloader)
    epoch_loss2 = running_loss2 / len(train_dataloader)
    epoch_total_loss = running_total_loss / len(train_dataloader)

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss1: {epoch_loss1:.4f}, Loss2: {epoch_loss2:.4f}, Total Loss: {epoch_total_loss:.4f}, Training  MSE: {epochMSE:.4f}')

    writer.add_scalar("Loss/train_loss1", epoch_loss1, epoch)
    writer.add_scalar("Loss/train_loss2", epoch_loss2, epoch)
    writer.add_scalar("Loss/total_loss", epoch_total_loss, epoch)
    writer.add_scalar("MSE/MSE_train", epochMSE, epoch)

    # VALIDATION
    model.eval()
    val_loss1 = 0.0
    val_loss2 = 0.0
    val_total_loss = 0.0

    with torch.no_grad():
        for batch in val_dataloader:
            coordinates = batch['coord'].to(torch.float32).to(device)
            segments = batch['segment'].to(torch.float32).to(device).transpose(1, 2)
            time_steps = batch['time'].to(torch.float32).to(device)

            inputs = torch.cat((coordinates, time_steps), dim=2).transpose(1, 2)

            outputs = model(inputs)

            loss1 = criterion1(outputs, segments)
            loss2 = navier_loss(model, inputs) if use_navier_loss else torch.tensor(0.0, device=device)
            loss = loss1 + loss2

            val_loss1 += loss1.item()
            val_loss2 += loss2.item()
            val_total_loss += loss.item()

            batchMSE.append(loss1.item())

        epochMSE = np.mean(batchMSE)
        valMSE.append(epochMSE)

    avg_val_loss1 = val_loss1 / len(val_dataloader)
    avg_val_loss2 = val_loss2 / len(val_dataloader)
    avg_val_total_loss = val_total_loss / len(val_dataloader)

    print(f'Validation Loss1: {avg_val_loss1:.4f}, Loss2: {avg_val_loss2:.4f}, Total Loss: {avg_val_total_loss:.4f}, Validation MSE: {epochMSE:.4f}')

    writer.add_scalar("Loss/val_loss1", avg_val_loss1, epoch)
    writer.add_scalar("Loss/val_loss2", avg_val_loss2, epoch)
    writer.add_scalar("Loss/val_total_loss", avg_val_total_loss, epoch)
    writer.add_scalar("MSE/val_MSE", epochMSE, epoch)

    checkpoint = {
        "epoch": epoch,
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
    }
    torch.save(checkpoint, checkpoint_path)
    print(f"Checkpoint saved at epoch {epoch+1}")
    writer.add_scalars('Loss/Epoch', {'train_loss1': epoch_loss1, 'val_loss1': avg_val_loss1}, epoch)
    writer.add_scalars('Loss/Epoch', {'train_loss2': epoch_loss2, 'val_loss2': avg_val_loss2}, epoch)
    writer.add_scalars('Loss/Epoch', {'train_total_loss': epoch_total_loss, 'val_total_loss': avg_val_total_loss}, epoch)

writer.close()
print("Training complete!")

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir=runs