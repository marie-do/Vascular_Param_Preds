# -*- coding: utf-8 -*-
"""Dataloader_skel_dataAug.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13vG5QGZbFIXYDn1b_xDF68u3mIez9QXQ
"""

from google.colab import drive
drive.mount('/content/drive')

import os

folder_path = "/content/drive/My Drive/data/Vascular"
cache_path = "/content/drive/My Drive/data/Vascular/cache/aorta"
skel_path = "/content/drive/My Drive/data/Vascular/skel/aorta"
files = os.listdir(skel_path)

print(files)

import os
import glob
import numpy as np
from copy import deepcopy
from torch.utils.data import Dataset
from collections.abc import Sequence


class DefaultDataset(Dataset):
    VALID_ASSETS = [
        "coord",
        "color",
        "normal",
        "strength",
        "segment",
        "instance",
        "pose",
    ]

    def __init__(
        self,
        split="train",
        data_root="data/dataset",
        transform=None,
        test_mode=False,
        test_cfg=None,
        cache=False,
        ignore_index=-1,
        loop=1,
    ):
        super(DefaultDataset, self).__init__()
        self.data_root = data_root
        self.split = split
        self.transform = Compose(transform)
        self.cache = cache
        self.ignore_index = ignore_index
        self.loop = (
            loop if not test_mode else 1
        )
        self.test_mode = test_mode
        self.test_cfg = test_cfg if test_mode else None

        if test_mode:
            self.test_voxelize = TRANSFORMS.build(self.test_cfg.voxelize)
            self.test_crop = (
                TRANSFORMS.build(self.test_cfg.crop) if self.test_cfg.crop else None
            )
            self.post_transform = Compose(self.test_cfg.post_transform)
            self.aug_transform = [Compose(aug) for aug in self.test_cfg.aug_transform]

        self.data_list = self.get_data_list()
        logger = get_root_logger()
        logger.info(
            "Totally {} x {} samples in {} set.".format(
                len(self.data_list), self.loop, split
            )
        )

    def get_data_list(self):
        if isinstance(self.split, str):
            data_list = glob.glob(os.path.join(self.data_root, self.split, "*"))
        elif isinstance(self.split, Sequence):
            data_list = []
            for split in self.split:
                data_list += glob.glob(os.path.join(self.data_root, split, "*"))
        else:
            raise NotImplementedError
        return data_list

    def get_data(self, idx):
        data_path = self.data_list[idx % len(self.data_list)]
        name = self.get_data_name(idx)
        if self.cache:
            cache_name = f"pointcept-{name}"
            return shared_dict(cache_name)

        data_dict = {}
        assets = os.listdir(data_path)
        for asset in assets:
            if not asset.endswith(".npy"):
                continue
            if asset[:-4] not in self.VALID_ASSETS:
                continue
            data_dict[asset[:-4]] = np.load(os.path.join(data_path, asset))
        data_dict["name"] = name

        if "coord" in data_dict.keys():
            data_dict["coord"] = data_dict["coord"].astype(np.float32)

        if "color" in data_dict.keys():
            data_dict["color"] = data_dict["color"].astype(np.float32)

        if "normal" in data_dict.keys():
            data_dict["normal"] = data_dict["normal"].astype(np.float32)

        if "segment" in data_dict.keys():
            data_dict["segment"] = data_dict["segment"].reshape([-1]).astype(np.int32)
        else:
            data_dict["segment"] = (
                np.ones(data_dict["coord"].shape[0], dtype=np.int32) * -1
            )

        if "instance" in data_dict.keys():
            data_dict["instance"] = data_dict["instance"].reshape([-1]).astype(np.int32)
        else:
            data_dict["instance"] = (
                np.ones(data_dict["coord"].shape[0], dtype=np.int32) * -1
            )
        return data_dict

    def get_data_name(self, idx):
        return os.path.basename(self.data_list[idx % len(self.data_list)])

    def prepare_train_data(self, idx):
        # load data
        data_dict = self.get_data(idx)
        data_dict = self.transform(data_dict)
        return data_dict

    def prepare_test_data(self, idx):
        # load data
        data_dict = self.get_data(idx)
        data_dict = self.transform(data_dict)
        result_dict = dict(segment=data_dict.pop("segment"), name=data_dict.pop("name"))
        if "origin_segment" in data_dict:
            assert "inverse" in data_dict
            result_dict["origin_segment"] = data_dict.pop("origin_segment")
            result_dict["inverse"] = data_dict.pop("inverse")

        data_dict_list = []
        for aug in self.aug_transform:
            data_dict_list.append(aug(deepcopy(data_dict)))

        fragment_list = []
        for data in data_dict_list:
            if self.test_voxelize is not None:
                data_part_list = self.test_voxelize(data)
            else:
                data["index"] = np.arange(data["coord"].shape[0])
                data_part_list = [data]
            for data_part in data_part_list:
                if self.test_crop is not None:
                    data_part = self.test_crop(data_part)
                else:
                    data_part = [data_part]
                fragment_list += data_part

        for i in range(len(fragment_list)):
            fragment_list[i] = self.post_transform(fragment_list[i])
        result_dict["fragment_list"] = fragment_list
        return result_dict

    def __getitem__(self, idx):
        if self.test_mode:
            return self.prepare_test_data(idx)
        else:
            return self.prepare_train_data(idx)

    def __len__(self):
        return len(self.data_list) * self.loop


class ConcatDataset(Dataset):
    def __init__(self, datasets, loop=1):
        super(ConcatDataset, self).__init__()
        self.datasets = [build_dataset(dataset) for dataset in datasets]
        self.loop = loop
        self.data_list = self.get_data_list()
        logger = get_root_logger()
        logger.info(
            "Totally {} x {} samples in the concat set.".format(
                len(self.data_list), self.loop
            )
        )

    def get_data_list(self):
        data_list = []
        for i in range(len(self.datasets)):
            data_list.extend(
                zip(
                    np.ones(len(self.datasets[i])) * i, np.arange(len(self.datasets[i]))
                )
            )
        return data_list

    def get_data(self, idx):
        dataset_idx, data_idx = self.data_list[idx % len(self.data_list)]
        return self.datasets[dataset_idx][data_idx]

    def get_data_name(self, idx):
        dataset_idx, data_idx = self.data_list[idx % len(self.data_list)]
        return self.datasets[dataset_idx].get_data_name(data_idx)

    def __getitem__(self, idx):
        return self.get_data(idx)

    def __len__(self):
        return len(self.data_list) * self.loop

!pip install open3d

def closest_distance_with_batch(p1, p2, is_sum=True):
    """
    :param p1: size[B,N,D]
    :param p2: size[B,M,D]
    :param is_sum: whehter to return the summed scalar or the separate distances with indices
    :return: the distances from p1 to the closest points in p2
    """
    assert p1.size(0) == p2.size(0) and p1.size(2) == p2.size(2)

    p1 = p1.unsqueeze(1)
    p2 = p2.unsqueeze(1)

    p1 = p1.repeat(1, p2.size(2), 1, 1)
    p1 = p1.transpose(1, 2)
    p2 = p2.repeat(1, p1.size(1), 1, 1)

    dist = torch.add(p1, torch.neg(p2))
    dist = torch.norm(dist, 2, dim=3)

    min_dist, min_indice = torch.min(dist, dim=2)
    dist_scalar = torch.sum(min_dist)

    if is_sum:
        return dist_scalar
    else:
        return min_dist, min_indice

import os
import numpy as np
import h5py
import open3d as o3d


class VascularDataset(DefaultDataset):
    def __init__(
            self,
            data_root="data/Vascular",
            category="aorta",
            point_number=512,
            num_points_skel=128,
            split="train",
            center_shift = False,
            random_scale = False,
            random_flip = False,
            random_jitter = False,
            normalize_coord = False,
            random_shift = False,
            **kwargs
    ):
        self.data_root = data_root
        self.category = category
        self.point_number = point_number
        self.num_points_skel = num_points_skel
        self.split = split
        # Data Augmentation
        self.center_shift = CenterShift(apply_z = True)
        self.random_scale = RandomScale(scale = [0.9,1.1],anisotropic = False)
        self.random_flip = RandomFlip()
        self.random_jitter = RandomJitter(sigma=0.01, clip=0.05)
        self.normalize_coord = NormalizeCoord()
        self.random_shift = RandomShift()

        super().__init__(data_root=self.data_root, split=self.split, **kwargs)
        self.vtu_path = os.path.join(self.data_root, 'vtu')
        self.cache_path = os.path.join(self.data_root, 'cache')
        self.skel_path = os.path.join(self.data_root, 'skel')
        self.split_path = os.path.join(self.data_root, 'splits', "{}.txt".format(self.split))
        self.data_list = np.loadtxt(self.split_path, dtype="str")
        if len(self.data_list.shape) == 0:
            self.data_list = np.expand_dims(self.data_list, 0)
        if self.category == 'all':
            self.data_list = self.data_list
        else:
            self.data_list = [entry for entry in self.data_list if entry.startswith(self.category)]

        self.time_steps = np.array(self.load_time_steps(), dtype=int)
        self.cumulative_time_steps = np.cumsum(self.time_steps)

    def load_time_steps(self):
        num_time_steps = []
        for data_name in self.data_list:
            category_number = data_name.split('-')[0]
            model_id = data_name.split('-')[-1].rstrip('\n')
            split_file_h5 = os.path.join(self.cache_path, category_number, f"{model_id}_all_fps{self.point_number}.h5")
            data = {}
            with h5py.File(split_file_h5, 'r') as f:
                for key in f.keys():
                    data[key] = f[key][()]
                split_data = len(data['time_step_data'])
                num_time_steps.append(split_data)
        return num_time_steps

    def __len__(self):
        return self.cumulative_time_steps[-1]

    def get_data_list(self):
        splits_path = os.path.join(self.data_root, 'splits')
        split_path = os.path.join(splits_path, "{}.txt".format(self.split))
        data_list = np.loadtxt(split_path, dtype="str")
        return data_list

    def get_data(self, index):
        file_idx = np.searchsorted(self.cumulative_time_steps, index, side='right')
        if file_idx > 0:
            time_step_idx = index - self.cumulative_time_steps[file_idx - 1]
        else:
            time_step_idx = index

        data_name = self.data_list[file_idx]
        category_number = data_name.split('-')[0]
        model_id = data_name.split('-')[-1].rstrip('\n')

        file_h5 = os.path.join(self.cache_path, category_number, f"{model_id}_all_fps{self.point_number}.h5")
        skel_path = os.path.join(self.skel_path, category_number, f"{model_id}.txt")

        data = {}
        with h5py.File(file_h5, 'r') as f:
            for key in f.keys():
                data[key] = f[key][()]

        skel_data = np.loadtxt(skel_path)
        skel = skel_data[:, :3]
        pcd = o3d.t.geometry.PointCloud()
        pcd.point["positions"] = o3d.core.Tensor(skel, dtype=o3d.core.Dtype.Float32)

        n_samples = min(self.num_points_skel, skel.shape[0])
        downpcd_farthest = pcd.farthest_point_down_sample(n_samples)
        skel = downpcd_farthest.point["positions"].numpy()

        name = data['file']
        file_name = name.decode('utf-8')
        name = file_name.split('.')[0]
        coordinates = data['coordinates']

        coordinates_unsqueezed = torch.Tensor(coordinates).unsqueeze(0)
        skel = torch.Tensor(skel).unsqueeze(0)
        min_dists_tuple = closest_distance_with_batch(
                 coordinates_unsqueezed,
                 skel,
                 is_sum=False
             )
        lfs, _ = min_dists_tuple

        time_step = data['time_step_data'][time_step_idx]
        time_step = round(time_step, 2)


        mean_pressure = data['pressure'].mean()
        std_pressure = data['pressure'].std()
        pressure = (data['pressure'][time_step_idx] - mean_pressure) / std_pressure

        mean_velocity = data['velocity'].mean()
        std_velocity = data['velocity'].std()
        velocity = (data['velocity'][time_step_idx] - mean_velocity) / std_velocity

        label_data = np.hstack((pressure.reshape(-1, 1), velocity))

        data_dict = dict(
            coord=coordinates,
            lfs=lfs,
            segment=label_data,
            time=(time_step * np.ones((coordinates.shape[0], 1))),
            name=name
        )

        if self.random_jitter:
          data_dict = self.random_jitter(data_dict)

        if self.random_scale:
          data_dict = self.random_scale(data_dict)

        if self.random_flip:
          data_dict = self.random_flip(data_dict)

        if self.center_shift:
          data_dict = self.center_shift(data_dict)

        if self.normalize_coord:
          data_dict = self.normalize_coord(data_dict)
        if self.random_shift:
          data_dict = self.random_shift(data_dict)

        return data_dict